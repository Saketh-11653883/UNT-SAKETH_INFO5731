{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saketh-11653883/UNT-SAKETH_INFO5731/blob/main/Kaveti_Saketh_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis of movie reviews is a intresting text categorization task. The objective of this work is to categorize reviews of movies as favorable, negative, or neutral depending on the text's tone.\n",
        "\n",
        "Characteristics for Emotion Evaluation in Film Reviews:\n",
        "\n",
        "Word Frequency:\n",
        "Analyzing a document's word frequency might reveal information about its general tone. Some words can convey either positive or negative feelings.\n",
        "Why it's beneficial? The sentiment of a review can be greatly influenced by the frequency of positive adjectives (like \"amazing,\" \"brilliant\") or negative ones (like \"disappointing,\" \"boring\").\n",
        "\n",
        "N-grams:\n",
        "Examining sets of related words (n-grams) can help comprehend sentiment intricacies and provide context.\n",
        "Why it's beneficial? Sentiment may be expressed more effectively by certain phrases or word combinations than by single words. For instance, although if the word \"good\" by itself is positive, saying it is \"not good\" might convey a negative opinion.\n",
        "\n",
        "Sentiment Lexicons:\n",
        "Using lexicons or dictionaries that correlate words with their sentiment polarity is known as sentiment lexicon usage.\n",
        "Why it's beneficial Lexicons aid in the identification of sentiment-bearing words and the polarities that go along with them, allowing for a more in-depth examination. Words like \"joyful\" are positive examples, but \"horrifying\" is negative.\n",
        "\n",
        "Part-of-Speech (POS) Tags:\n",
        "Classifying words according to their part of speech (nouns, verbs, adjectives) in order to analyze the grammatical structure.\n",
        "Why it's beneficial? The words that are utilized in a statement might affect how it feels. Finding and detecting adjectives and adverbs helps improve the model's accuracy because they frequently play a significant role in expressing sentiment.\n",
        "\n",
        "\n",
        "Negation Handling:\n",
        "To identify negation words and evaluate how they affect the sentiment of the words that follow.\n",
        "Why it's beneficial? \"Not good\" and other such phrases convey a different meaning than \"good\" alone. By assisting the model in identifying when a word is negated, negative handling enhances the accuracy of the sentiment analysis.\n",
        "\n",
        "\n",
        "When combined, these qualities offer a complete understanding of the emotions expressed in film reviews.\n",
        "The system may be trained to identify relationships and patterns in the text by incorporating them into a machine learning model, which would raise the accuracy of the sentiment categorization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XyT6rlL_2R07"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3014d73-449d-4a20-8d58-9494c2e36a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Review: This movie is absolutely amazing! The acting and plot are fantastic.\n",
            "Word Frequency: <FreqDist with 13 samples and 13 outcomes>\n",
            "N-grams: [('this', 'movie'), ('movie', 'is'), ('is', 'absolutely'), ('absolutely', 'amazing'), ('amazing', '!'), ('!', 'the'), ('the', 'acting'), ('acting', 'and'), ('and', 'plot'), ('plot', 'are'), ('are', 'fantastic'), ('fantastic', '.')]\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.53, 'pos': 0.47, 'compound': 0.8395}\n",
            "POS Tags: [('this', 'DT'), ('movie', 'NN'), ('is', 'VBZ'), ('absolutely', 'RB'), ('amazing', 'JJ'), ('!', '.'), ('the', 'DT'), ('acting', 'NN'), ('and', 'CC'), ('plot', 'NN'), ('are', 'VBP'), ('fantastic', 'JJ'), ('.', '.')]\n",
            "Negation Handling: this movie is absolutely amazing ! the acting and plot are fantastic .\n",
            "\n",
            "Review: I was really disappointed with the film. It was boring and predictable.\n",
            "Word Frequency: <FreqDist with 12 samples and 14 outcomes>\n",
            "N-grams: [('i', 'was'), ('was', 'really'), ('really', 'disappointed'), ('disappointed', 'with'), ('with', 'the'), ('the', 'film'), ('film', '.'), ('.', 'it'), ('it', 'was'), ('was', 'boring'), ('boring', 'and'), ('and', 'predictable'), ('predictable', '.')]\n",
            "Sentiment Score: {'neg': 0.387, 'neu': 0.613, 'pos': 0.0, 'compound': -0.6901}\n",
            "POS Tags: [('i', 'NN'), ('was', 'VBD'), ('really', 'RB'), ('disappointed', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('film', 'NN'), ('.', '.'), ('it', 'PRP'), ('was', 'VBD'), ('boring', 'VBG'), ('and', 'CC'), ('predictable', 'JJ'), ('.', '.')]\n",
            "Negation Handling: i was really disappointed with the film . it was boring and predictable .\n",
            "\n",
            "Review: The movie didn't live up to my expectations. It was not good at all.\n",
            "Word Frequency: <FreqDist with 16 samples and 17 outcomes>\n",
            "N-grams: [('the', 'movie'), ('movie', 'did'), ('did', \"n't\"), (\"n't\", 'live'), ('live', 'up'), ('up', 'to'), ('to', 'my'), ('my', 'expectations'), ('expectations', '.'), ('.', 'it'), ('it', 'was'), ('was', 'not'), ('not', 'good'), ('good', 'at'), ('at', 'all'), ('all', '.')]\n",
            "Sentiment Score: {'neg': 0.156, 'neu': 0.844, 'pos': 0.0, 'compound': -0.3412}\n",
            "POS Tags: [('the', 'DT'), ('movie', 'NN'), ('did', 'VBD'), (\"n't\", 'RB'), ('live', 'VB'), ('up', 'RB'), ('to', 'TO'), ('my', 'PRP$'), ('expectations', 'NNS'), ('.', '.'), ('it', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('good', 'JJ'), ('at', 'IN'), ('all', 'DT'), ('.', '.')]\n",
            "Negation Handling: the movie did n't live up to my expectations . it was not_good not_at not_all not_.\n",
            "\n",
            "Review: An intriguing plot and great characters make this movie a must-watch.\n",
            "Word Frequency: <FreqDist with 12 samples and 12 outcomes>\n",
            "N-grams: [('an', 'intriguing'), ('intriguing', 'plot'), ('plot', 'and'), ('and', 'great'), ('great', 'characters'), ('characters', 'make'), ('make', 'this'), ('this', 'movie'), ('movie', 'a'), ('a', 'must-watch'), ('must-watch', '.')]\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.687, 'pos': 0.313, 'compound': 0.6249}\n",
            "POS Tags: [('an', 'DT'), ('intriguing', 'JJ'), ('plot', 'NN'), ('and', 'CC'), ('great', 'JJ'), ('characters', 'NNS'), ('make', 'VBP'), ('this', 'DT'), ('movie', 'NN'), ('a', 'DT'), ('must-watch', 'NN'), ('.', '.')]\n",
            "Negation Handling: an intriguing plot and great characters make this movie a must-watch .\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Sample movie reviews\n",
        "reviews = [\n",
        "    \"This movie is absolutely amazing! The acting and plot are fantastic.\",\n",
        "    \"I was really disappointed with the film. It was boring and predictable.\",\n",
        "    \"The movie didn't live up to my expectations. It was not good at all.\",\n",
        "    \"An intriguing plot and great characters make this movie a must-watch.\",\n",
        "]\n",
        "\n",
        "# Feature 1: Word Frequency\n",
        "def get_word_frequency(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    word_freq = nltk.FreqDist(words)\n",
        "    return word_freq\n",
        "\n",
        "# Feature 2: N-grams\n",
        "def get_ngrams(text, n=2):\n",
        "    words = word_tokenize(text.lower())\n",
        "    n_grams = list(ngrams(words, n))\n",
        "    return n_grams\n",
        "\n",
        "# Feature 3: Sentiment Lexicons\n",
        "def get_sentiment_score(text):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment_score = sia.polarity_scores(text)\n",
        "    return sentiment_score\n",
        "\n",
        "# Feature 4: Part-of-Speech Tags\n",
        "def get_pos_tags(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    pos_tags = pos_tag(words)\n",
        "    return pos_tags\n",
        "\n",
        "# Feature 5: Negation Handling\n",
        "def handle_negation(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    negation_words = set([\"not\", \"no\", \"never\"])\n",
        "\n",
        "    negated_text = []\n",
        "    negate = False\n",
        "    for word in words:\n",
        "        if word in negation_words:\n",
        "            negate = not negate\n",
        "        else:\n",
        "            negated_text.append(\"not_\" + word if negate else word)\n",
        "\n",
        "    return \" \".join(negated_text)\n",
        "\n",
        "# Apply the functions to each review\n",
        "for review in reviews:\n",
        "    print(\"\\nReview:\", review)\n",
        "\n",
        "    # Feature 1: Word Frequency\n",
        "    word_frequency = get_word_frequency(review)\n",
        "    print(\"Word Frequency:\", word_frequency)\n",
        "\n",
        "    # Feature 2: N-grams\n",
        "    n_grams = get_ngrams(review)\n",
        "    print(\"N-grams:\", n_grams)\n",
        "\n",
        "    # Feature 3: Sentiment Lexicons\n",
        "    sentiment_score = get_sentiment_score(review)\n",
        "    print(\"Sentiment Score:\", sentiment_score)\n",
        "\n",
        "    # Feature 4: Part-of-Speech Tags\n",
        "    pos_tags = get_pos_tags(review)\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "    # Feature 5: Negation Handling\n",
        "    negated_review = handle_negation(review)\n",
        "    print(\"Negation Handling:\", negated_review)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "reviews = [\n",
        "    \"Feature Selection (FS) methods alleviate key problems in classification procedures as they are used to improve classification accuracy, reduce data dimensionality, and remove irrelevant data.\",\n",
        "    \"FS methods have received a great deal of attention from the text classification community. However, only a few literature surveys include them focusing on text classification, and the ones available are either a superficial analysis or present a very small set of work in the subject.\",\n",
        "    \"For this reason, we conducted a Systematic Literature Review (SLR) that assesses 1376 unique papers from journals and conferences published in the past eight years (2013–2020). After abstract screening and full-text eligibility analysis, 175 studies were included in our SLR.\",\n",
        "    \"Our contribution is twofold. We have considered several aspects of each proposed method and mapped them into a new categorization schema. Additionally, we mapped the main characteristics of the experiments, identifying which datasets, languages, machine learning algorithms, and validation methods have been used to evaluate new and existing techniques.\",\n",
        "    \"By following the SLR protocol, we allow the replication of our revision process and minimize the chances of bias while classifying the included studies. By mapping issues and experiment settings, our SLR helps researchers to develop and position new studies with respect to the existing literature.\",\n",
        "    \"Keywords Feature selection · Dimensionality reduction · Text classification · Systematic literature review\"\n",
        "]\n",
        "\n",
        "# Labels for the text classification task (assuming two classes for simplicity)\n",
        "labels = [0, 1, 0, 1, 0, 1]\n",
        "\n",
        "# Convert text data to numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_transformed = vectorizer.fit_transform(reviews)\n",
        "\n",
        "# Use Chi-square for feature selection\n",
        "chi2_selector = SelectKBest(chi2, k=5)  # Select top 5 features\n",
        "X_chi2_selected = chi2_selector.fit_transform(X_transformed, labels)\n",
        "\n",
        "# Get feature names from the CountVectorizer\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Create a dictionary mapping feature names to their importance scores\n",
        "feature_chi2_scores = {feature: score for feature, score in zip(feature_names, chi2_selector.scores_)}\n",
        "\n",
        "# Rank features by importance in descending order\n",
        "sorted_features = sorted(feature_chi2_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display the selected features and their importance scores\n",
        "print(\"\\nRanked Features based on Chi-square Importance:\")\n",
        "for feature, score in sorted_features:\n",
        "    print(f\"Feature: {feature}, Chi-square Score: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6gw-3-e-QnZ",
        "outputId": "3ab28f01-74bc-4c49-fd0e-08a822c100a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ranked Features based on Chi-square Importance:\n",
            "Feature: slr, Chi-square Score: 4.0000\n",
            "Feature: have, Chi-square Score: 3.0000\n",
            "Feature: studies, Chi-square Score: 3.0000\n",
            "Feature: by, Chi-square Score: 2.0000\n",
            "Feature: data, Chi-square Score: 2.0000\n",
            "Feature: included, Chi-square Score: 2.0000\n",
            "Feature: mapped, Chi-square Score: 2.0000\n",
            "Feature: them, Chi-square Score: 2.0000\n",
            "Feature: 1376, Chi-square Score: 1.0000\n",
            "Feature: 175, Chi-square Score: 1.0000\n",
            "Feature: 2013, Chi-square Score: 1.0000\n",
            "Feature: 2020, Chi-square Score: 1.0000\n",
            "Feature: abstract, Chi-square Score: 1.0000\n",
            "Feature: accuracy, Chi-square Score: 1.0000\n",
            "Feature: additionally, Chi-square Score: 1.0000\n",
            "Feature: after, Chi-square Score: 1.0000\n",
            "Feature: algorithms, Chi-square Score: 1.0000\n",
            "Feature: alleviate, Chi-square Score: 1.0000\n",
            "Feature: allow, Chi-square Score: 1.0000\n",
            "Feature: as, Chi-square Score: 1.0000\n",
            "Feature: aspects, Chi-square Score: 1.0000\n",
            "Feature: assesses, Chi-square Score: 1.0000\n",
            "Feature: attention, Chi-square Score: 1.0000\n",
            "Feature: available, Chi-square Score: 1.0000\n",
            "Feature: been, Chi-square Score: 1.0000\n",
            "Feature: bias, Chi-square Score: 1.0000\n",
            "Feature: categorization, Chi-square Score: 1.0000\n",
            "Feature: chances, Chi-square Score: 1.0000\n",
            "Feature: characteristics, Chi-square Score: 1.0000\n",
            "Feature: classifying, Chi-square Score: 1.0000\n",
            "Feature: community, Chi-square Score: 1.0000\n",
            "Feature: conducted, Chi-square Score: 1.0000\n",
            "Feature: conferences, Chi-square Score: 1.0000\n",
            "Feature: considered, Chi-square Score: 1.0000\n",
            "Feature: contribution, Chi-square Score: 1.0000\n",
            "Feature: datasets, Chi-square Score: 1.0000\n",
            "Feature: deal, Chi-square Score: 1.0000\n",
            "Feature: develop, Chi-square Score: 1.0000\n",
            "Feature: each, Chi-square Score: 1.0000\n",
            "Feature: eight, Chi-square Score: 1.0000\n",
            "Feature: either, Chi-square Score: 1.0000\n",
            "Feature: eligibility, Chi-square Score: 1.0000\n",
            "Feature: evaluate, Chi-square Score: 1.0000\n",
            "Feature: experiment, Chi-square Score: 1.0000\n",
            "Feature: experiments, Chi-square Score: 1.0000\n",
            "Feature: few, Chi-square Score: 1.0000\n",
            "Feature: focusing, Chi-square Score: 1.0000\n",
            "Feature: following, Chi-square Score: 1.0000\n",
            "Feature: for, Chi-square Score: 1.0000\n",
            "Feature: full, Chi-square Score: 1.0000\n",
            "Feature: great, Chi-square Score: 1.0000\n",
            "Feature: helps, Chi-square Score: 1.0000\n",
            "Feature: however, Chi-square Score: 1.0000\n",
            "Feature: identifying, Chi-square Score: 1.0000\n",
            "Feature: improve, Chi-square Score: 1.0000\n",
            "Feature: in, Chi-square Score: 1.0000\n",
            "Feature: include, Chi-square Score: 1.0000\n",
            "Feature: into, Chi-square Score: 1.0000\n",
            "Feature: irrelevant, Chi-square Score: 1.0000\n",
            "Feature: is, Chi-square Score: 1.0000\n",
            "Feature: issues, Chi-square Score: 1.0000\n",
            "Feature: journals, Chi-square Score: 1.0000\n",
            "Feature: key, Chi-square Score: 1.0000\n",
            "Feature: keywords, Chi-square Score: 1.0000\n",
            "Feature: languages, Chi-square Score: 1.0000\n",
            "Feature: learning, Chi-square Score: 1.0000\n",
            "Feature: machine, Chi-square Score: 1.0000\n",
            "Feature: main, Chi-square Score: 1.0000\n",
            "Feature: mapping, Chi-square Score: 1.0000\n",
            "Feature: method, Chi-square Score: 1.0000\n",
            "Feature: minimize, Chi-square Score: 1.0000\n",
            "Feature: on, Chi-square Score: 1.0000\n",
            "Feature: ones, Chi-square Score: 1.0000\n",
            "Feature: only, Chi-square Score: 1.0000\n",
            "Feature: or, Chi-square Score: 1.0000\n",
            "Feature: our, Chi-square Score: 1.0000\n",
            "Feature: papers, Chi-square Score: 1.0000\n",
            "Feature: past, Chi-square Score: 1.0000\n",
            "Feature: position, Chi-square Score: 1.0000\n",
            "Feature: present, Chi-square Score: 1.0000\n",
            "Feature: problems, Chi-square Score: 1.0000\n",
            "Feature: procedures, Chi-square Score: 1.0000\n",
            "Feature: process, Chi-square Score: 1.0000\n",
            "Feature: proposed, Chi-square Score: 1.0000\n",
            "Feature: protocol, Chi-square Score: 1.0000\n",
            "Feature: published, Chi-square Score: 1.0000\n",
            "Feature: reason, Chi-square Score: 1.0000\n",
            "Feature: received, Chi-square Score: 1.0000\n",
            "Feature: reduce, Chi-square Score: 1.0000\n",
            "Feature: reduction, Chi-square Score: 1.0000\n",
            "Feature: remove, Chi-square Score: 1.0000\n",
            "Feature: replication, Chi-square Score: 1.0000\n",
            "Feature: researchers, Chi-square Score: 1.0000\n",
            "Feature: respect, Chi-square Score: 1.0000\n",
            "Feature: revision, Chi-square Score: 1.0000\n",
            "Feature: schema, Chi-square Score: 1.0000\n",
            "Feature: screening, Chi-square Score: 1.0000\n",
            "Feature: set, Chi-square Score: 1.0000\n",
            "Feature: settings, Chi-square Score: 1.0000\n",
            "Feature: several, Chi-square Score: 1.0000\n",
            "Feature: small, Chi-square Score: 1.0000\n",
            "Feature: subject, Chi-square Score: 1.0000\n",
            "Feature: superficial, Chi-square Score: 1.0000\n",
            "Feature: surveys, Chi-square Score: 1.0000\n",
            "Feature: techniques, Chi-square Score: 1.0000\n",
            "Feature: text, Chi-square Score: 1.0000\n",
            "Feature: that, Chi-square Score: 1.0000\n",
            "Feature: they, Chi-square Score: 1.0000\n",
            "Feature: this, Chi-square Score: 1.0000\n",
            "Feature: to, Chi-square Score: 1.0000\n",
            "Feature: twofold, Chi-square Score: 1.0000\n",
            "Feature: unique, Chi-square Score: 1.0000\n",
            "Feature: validation, Chi-square Score: 1.0000\n",
            "Feature: very, Chi-square Score: 1.0000\n",
            "Feature: were, Chi-square Score: 1.0000\n",
            "Feature: which, Chi-square Score: 1.0000\n",
            "Feature: while, Chi-square Score: 1.0000\n",
            "Feature: with, Chi-square Score: 1.0000\n",
            "Feature: work, Chi-square Score: 1.0000\n",
            "Feature: years, Chi-square Score: 1.0000\n",
            "Feature: of, Chi-square Score: 0.6667\n",
            "Feature: and, Chi-square Score: 0.4000\n",
            "Feature: methods, Chi-square Score: 0.3333\n",
            "Feature: new, Chi-square Score: 0.3333\n",
            "Feature: classification, Chi-square Score: 0.2000\n",
            "Feature: the, Chi-square Score: 0.0909\n",
            "Feature: analysis, Chi-square Score: 0.0000\n",
            "Feature: are, Chi-square Score: 0.0000\n",
            "Feature: dimensionality, Chi-square Score: 0.0000\n",
            "Feature: existing, Chi-square Score: 0.0000\n",
            "Feature: feature, Chi-square Score: 0.0000\n",
            "Feature: from, Chi-square Score: 0.0000\n",
            "Feature: fs, Chi-square Score: 0.0000\n",
            "Feature: literature, Chi-square Score: 0.0000\n",
            "Feature: review, Chi-square Score: 0.0000\n",
            "Feature: selection, Chi-square Score: 0.0000\n",
            "Feature: systematic, Chi-square Score: 0.0000\n",
            "Feature: used, Chi-square Score: 0.0000\n",
            "Feature: we, Chi-square Score: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample movie reviews\n",
        "reviews = [\n",
        "    \"This movie is absolutely amazing! The acting and plot are fantastic.\",\n",
        "    \"I was really disappointed with the film. It was boring and predictable.\",\n",
        "    \"The movie didn't live up to my expectations. It was not good at all.\",\n",
        "    \"An intriguing plot and great characters make this movie a must-watch.\",\n",
        "]\n",
        "\n",
        "# Sample queries for testing\n",
        "queries = [\n",
        "    \"I'm looking for a movie with a captivating plot and strong characters.\",\n",
        "    \"Any recommendations for a comedy with a lot of humor?\",\n",
        "    \"Looking for a mystery thriller that will keep me guessing until the end.\",\n",
        "    \"What's the best animated movie for kids?\",\n",
        "]\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to get BERT embeddings for a given text\n",
        "def get_bert_embeddings(text):\n",
        "    tokens = tokenizer(text, return_tensors='pt')\n",
        "    outputs = model(**tokens)\n",
        "    return outputs.pooler_output.detach().numpy()\n",
        "\n",
        "# Calculate BERT embeddings for each review\n",
        "reviews_embeddings = [get_bert_embeddings(review) for review in reviews]\n",
        "\n",
        "# Compare each query with movie reviews\n",
        "for query in queries:\n",
        "    print(f\"\\nQuery: {query}\\n\")\n",
        "\n",
        "    # Calculate BERT embeddings for the query\n",
        "    query_embedding = get_bert_embeddings(query)\n",
        "\n",
        "    # Calculate cosine similarity between the query and each review\n",
        "    similarities = [cosine_similarity(query_embedding, review_embedding.reshape(1, -1))[0][0] for review_embedding in reviews_embeddings]\n",
        "\n",
        "    # Rank the reviews based on similarity in descending order\n",
        "    ranked_reviews = sorted(zip(reviews, similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Display the ranked reviews\n",
        "    print(\"Ranked Reviews based on Cosine Similarity:\")\n",
        "    for idx, (review, similarity) in enumerate(ranked_reviews, start=1):\n",
        "        print(f\"{idx}. Similarity: {similarity:.4f}\\n   Review: {review}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHKiexyX6zLJ",
        "outputId": "cf44dcc4-12d2-4975-af01-bdfe1d01b653"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: I'm looking for a movie with a captivating plot and strong characters.\n",
            "\n",
            "Ranked Reviews based on Cosine Similarity:\n",
            "1. Similarity: 0.9712\n",
            "   Review: I was really disappointed with the film. It was boring and predictable.\n",
            "\n",
            "2. Similarity: 0.9489\n",
            "   Review: An intriguing plot and great characters make this movie a must-watch.\n",
            "\n",
            "3. Similarity: 0.9323\n",
            "   Review: The movie didn't live up to my expectations. It was not good at all.\n",
            "\n",
            "4. Similarity: 0.8864\n",
            "   Review: This movie is absolutely amazing! The acting and plot are fantastic.\n",
            "\n",
            "\n",
            "Query: Any recommendations for a comedy with a lot of humor?\n",
            "\n",
            "Ranked Reviews based on Cosine Similarity:\n",
            "1. Similarity: 0.9846\n",
            "   Review: The movie didn't live up to my expectations. It was not good at all.\n",
            "\n",
            "2. Similarity: 0.9772\n",
            "   Review: This movie is absolutely amazing! The acting and plot are fantastic.\n",
            "\n",
            "3. Similarity: 0.9639\n",
            "   Review: An intriguing plot and great characters make this movie a must-watch.\n",
            "\n",
            "4. Similarity: 0.9294\n",
            "   Review: I was really disappointed with the film. It was boring and predictable.\n",
            "\n",
            "\n",
            "Query: Looking for a mystery thriller that will keep me guessing until the end.\n",
            "\n",
            "Ranked Reviews based on Cosine Similarity:\n",
            "1. Similarity: 0.9692\n",
            "   Review: The movie didn't live up to my expectations. It was not good at all.\n",
            "\n",
            "2. Similarity: 0.9667\n",
            "   Review: I was really disappointed with the film. It was boring and predictable.\n",
            "\n",
            "3. Similarity: 0.9665\n",
            "   Review: An intriguing plot and great characters make this movie a must-watch.\n",
            "\n",
            "4. Similarity: 0.9219\n",
            "   Review: This movie is absolutely amazing! The acting and plot are fantastic.\n",
            "\n",
            "\n",
            "Query: What's the best animated movie for kids?\n",
            "\n",
            "Ranked Reviews based on Cosine Similarity:\n",
            "1. Similarity: 0.9671\n",
            "   Review: The movie didn't live up to my expectations. It was not good at all.\n",
            "\n",
            "2. Similarity: 0.9476\n",
            "   Review: An intriguing plot and great characters make this movie a must-watch.\n",
            "\n",
            "3. Similarity: 0.9385\n",
            "   Review: I was really disappointed with the film. It was boring and predictable.\n",
            "\n",
            "4. Similarity: 0.9308\n",
            "   Review: This movie is absolutely amazing! The acting and plot are fantastic.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After completing this exercises, I have a practical understanding of how text classification and feature extraction are accomplished using machine learning and natural language processing (NLP) approaches. The key ideas were comprehending various feature extraction methods, feature selection strategies, and text similarity ranking with BERT embeddings.To use BERT embeddings for text similarity, one must have a solid understanding of tokenization techniques and the Hugging Face Transformers library.Testing and subject knowledge are required to select the most relevant features for a text categorization assignment.These exercises generally aid in comprehension of NLP principles and ability to use them in practical situations.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5S3etOGVlibc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
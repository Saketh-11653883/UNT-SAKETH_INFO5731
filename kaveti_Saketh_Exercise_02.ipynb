{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saketh-11653883/UNT-SAKETH_INFO5731/blob/main/kaveti_Saketh_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qustion: How does the attitude reflected in students' study comments connect to their academic success, and what insights may this correlation offer for improving educational support systems?\n",
        "\n",
        "The research question seeks to determine whether there is a link between the attitudes exhibited in students' comments about their study experiences and their academic success. We can learn about how students' emotional experiences affect their academic performance by assessing sentiment in study comments and connecting it to exam scores.\n",
        "\n",
        "Data Needed:\n",
        "\n",
        "Collect text data about student remarks on their study experiences. This information can be acquired through surveys, forums, or any other venue where students can express their opinions.\n",
        "\n",
        "Analyze student comments based on their sentiment. This could be done with Natural Language Processing NLP software or libraries.\n",
        "\n",
        "Collect data on students' academic performance, including exam scores and grades.\n",
        "\n",
        "Include demographic information like age, gender, and academic year to compare sentiment patterns among student groupings.\n",
        "\n",
        "Amount of data:\n",
        "\n",
        "Collect data from at least 500-2000 pupils to conduct a full study.Ensure that students are represented diversely in order to reflect a wide range of sentiments and academic performance.\n",
        "\n",
        "steps for collecting and storing data:\n",
        "\n",
        "Design a survey or utilize a platform where students can voluntarily share comments about their study experiences. Ensure that the survey includes questions about demographic information such as age, gender, and academic year.\n",
        "\n",
        "Once the data collection is complete, perform sentiment analysis on the textual responses using Natural Language Processing NLP techniques or libraries to extract sentiments. Simultaneously, gather data on academic performance, such as exam scores or grades, for each student. This information can be obtained from academic records or assessments.\n",
        "\n",
        "Collate all collected data, including student IDs, comments, sentiments, academic performance metrics, and demographic details, into a structured dataset. Utilize a tool like pandas in Python to create a DataFrame and save the dataset in CSV format.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "llqnBtTwlb69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize empty lists for data\n",
        "student_id_data = []\n",
        "comments_data = []\n",
        "sentiment_data = []\n",
        "exam_score_data = []\n",
        "age_data = []\n",
        "gender_data = []\n",
        "\n",
        "# Generate random data for 1000 students\n",
        "for student_id in range(1, 1001):\n",
        "    # Generate random student comments\n",
        "    student_comment_length = random.randint(5, 50)\n",
        "    student_comment = ' '.join(random.choices(['I', 'like', 'enjoy', 'struggle', 'study', 'exams'], k=student_comment_length))\n",
        "\n",
        "    # Perform sentiment analysis (randomly assign sentiment for demonstration purposes)\n",
        "    sentiment = random.choice(['Positive', 'Neutral', 'Negative'])\n",
        "\n",
        "    # Generate random exam scores\n",
        "    exam_score = random.uniform(40, 100)\n",
        "\n",
        "    # Append data to respective lists\n",
        "    student_id_data.append(student_id)\n",
        "    comments_data.append(student_comment)\n",
        "    sentiment_data.append(sentiment)\n",
        "    exam_score_data.append(exam_score)\n",
        "    age_data.append(random.randint(18, 25))\n",
        "    gender_data.append(random.choice(['Male', 'Female']))\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Student ID': student_id_data,\n",
        "    'Comments': comments_data,\n",
        "    'Sentiment': sentiment_data,\n",
        "    'Exam Score': exam_score_data,\n",
        "    'Age': age_data,\n",
        "    'Gender': gender_data\n",
        "})\n",
        "\n",
        "# Save the data to a CSV file\n",
        "data.to_csv('sentiment_analysis_and_performance.csv', index=False)\n",
        "\n",
        "print(\"Data collection and saving complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AKJ6ECdIF4f",
        "outputId": "87090ac4-4a3d-42f4-f47d-f2eb0e7b42b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data collection and saving complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required modules.\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Creating the variable topic with the value as the keyword we need to search for in the Semantic Scholar articles.\n",
        "topic = 'Analysis'\n",
        "\n",
        "# Storing the URL variable with dynamic query keyword you need to search for and different query parameters.\n",
        "url = \"https://api.semanticscholar.org/graph/v1/paper/search?query=\"+ topic + \"&offset=100&limit=3&fields=title,year,venue,abstract,authors\"\n",
        "\n",
        "# Creating the empty payload and headers dictionary, including the API key.\n",
        "payload = {}\n",
        "headers = {\n",
        "    'X-SemanticScholar-API-Key': 'GQUb5nQic4aos3G1A77cf4Yel5kxFFiN8oMhUuDX'\n",
        "}\n",
        "\n",
        "# Fetch 1000 articles in batches of 100\n",
        "all_articles = []\n",
        "\n",
        "for offset in range(0, 1000, 100):\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/paper/search?query=\"+ topic + f\"&offset={offset}&limit=100&fields=title,year,venue,abstract,authors\"\n",
        "\n",
        "    # Hitting the GET API using the request module.\n",
        "    response = requests.get(url, headers=headers, data=payload)\n",
        "\n",
        "    # Check if the response status code indicates a successful request (status code 200 OK).\n",
        "    if response.status_code == 200:\n",
        "        # Load the response text as JSON\n",
        "        response_json = response.json()\n",
        "\n",
        "        # Check if the 'data' key is present in the JSON response\n",
        "        if 'data' in response_json:\n",
        "            # Extract the data\n",
        "            data = response_json['data']\n",
        "\n",
        "            # Extend the list with the data from the current batch\n",
        "            all_articles.extend(data)\n",
        "        else:\n",
        "            print(f\"No 'data' key found in the JSON response for offset {offset}.\")\n",
        "    else:\n",
        "        print(f\"Request failed with status code {response.status_code} for offset {offset}.\")\n",
        "        print(response.text)  # Print the response text for further inspection\n",
        "\n",
        " # Sleep for a few seconds to avoid hitting rate limits\n",
        "    time.sleep(10)  # You can adjust the sleep duration based on the rate limits\n",
        "\n",
        "\n",
        "# Create the pandas DataFrame object using the accumulated data\n",
        "df = pd.DataFrame(all_articles)\n",
        "\n",
        "# Save the DataFrame to a JSON file\n",
        "df.to_json('semantic_scholar_output.json', orient='records', lines=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jBvTAZVnbVM",
        "outputId": "5f6e5ef4-5c92-4711-caf0-0225b3d348b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request failed with status code 429 for offset 0.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Request failed with status code 429 for offset 200.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Request failed with status code 429 for offset 300.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Request failed with status code 429 for offset 400.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Request failed with status code 429 for offset 500.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Request failed with status code 429 for offset 600.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Request failed with status code 429 for offset 700.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Request failed with status code 429 for offset 800.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n",
            "Request failed with status code 429 for offset 900.\n",
            "{\"message\": \"Too Many Requests. Please wait and try again or apply for a key for higher rate limits. https://www.semanticscholar.org/product/api#api-key-form\", \"code\": \"429\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ],
      "metadata": {
        "id": "oPdqlAUY4THS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tweepy\n",
        "import csv\n",
        "import pandas as pd\n",
        "# input your credentials here\n",
        "consumer_key = 'DcsFKeZ5XeGcQQsPHTM9agR1a'\n",
        "consumer_secret = '7G5inABe6isxOSsI8T8DzUkS2fyiqrxAPb76ragjDriPtxaWX3'\n",
        "access_token = '1868115330-7nkRfvlyCK6CvSXaRUQJQx1ykLTQ7ROiTMTBY6m'\n",
        "access_token_secret = 'KMNy8jpTt62KFlNfsH6GEKACmANAkIilwqTmZrcxEUpyW' # you need to get these details from twitter developer account.\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "# Open/Create a file to append data\n",
        "csvFile = open('airlines.csv', 'a')\n",
        "#Use csv Writer\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "# Fetch tweets containing the specified query\n",
        "for tweet in tweepy.Cursor(api.search_tweets, q=query, lang='en', tweet_mode='extended').items(1000):\n",
        "    # Write relevant information to the CSV file\n",
        "    csvWriter.writerow([tweet.created_at, tweet.user.screen_name, tweet.full_text])\n",
        "\n",
        "# Close the CSV file\n",
        "csvFile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "JSOj7lBK4ZVy",
        "outputId": "19c5c8b7-692a-46e1-b16b-80b20a57d5fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'query' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bb1cd23ae7a7>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Fetch tweets containing the specified query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'extended'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Write relevant information to the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcsvWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working on web scraping activities has been an excellent learning opportunity. One of the most important lessons I took away was the significance of understanding HTML structure and using frameworks like BeautifulSoup and Python requests to explore and retrieve useful data. Learning how to evaluate website elements, discover unique identifiers, and manage dynamic material has been critical.\n",
        "\n",
        "When attempting to scrape data from websites that required authentication or API keys, challenges were encountered. Obtaining credentials for scraping, especially from platforms like Twitter or Instagram, involved navigating through developer portals, understanding rate limits, and adhering to terms of service. Overcoming these challenges required a balance of compliance with platform policies and effective data extraction.\n",
        "\n",
        "In my subject of study, data science, the capacity to collect and evaluate data from online sources is critical. It enables detailed literature reviews, sentiment analysis, and trend identification. Web scraping provides valuable insights to research and decision-making processes, resulting in a more sophisticated understanding of the topic matter."
      ],
      "metadata": {
        "id": "cXARywlaOfY8"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}